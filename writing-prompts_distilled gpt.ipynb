{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Importing Libraies\n"]},{"cell_type":"markdown","metadata":{},"source":["The code snippet begins with a series of import statements, each serving a specific purpose in facilitating the subsequent tasks related to natural language processing (NLP) and model training. Firstly, the Dataset class is imported from the datasets library, which is part of the Hugging Face ecosystem. This class provides functionalities for handling datasets, including loading, processing, and manipulating data. Following this, the DataCollatorWithPadding class is imported from the transformers library. This class is instrumental in preparing input data for model training by padding sequences to ensure uniform length, a crucial step for batch processing.\n","\n","Next, several essential components for fine-tuning a pre-trained GPT-2 model are imported from the transformers library. These include the Trainer and TrainingArguments classes, which facilitate the training process by defining training parameters and orchestrating the training loop, respectively. Additionally, the GPT2LMHeadModel and GPT2Tokenizer classes are imported, representing the pre-trained GPT-2 model architecture and tokenizer, respectively. These components are pivotal for fine-tuning the GPT-2 model on custom data and generating text.\n","\n","Continuing with the imports, the code brings in the PorterStemmer class from the nltk.stem module. Stemming, a technique for reducing words to their root form, can aid in preprocessing textual data by normalizing word variations. Additionally, the SnowballStemmer class is imported from the same module, offering another stemming algorithm for text normalization. Furthermore, the string module is imported to access utility functions for string manipulation, such as handling punctuation.\n","\n","The code also imports essential libraries for data manipulation and analysis. Specifically, the torch library is imported for tensor computations, which are fundamental for neural network operations. Additionally, the pandas library is imported for efficient data manipulation, particularly for working with tabular data structures such as DataFrames. Moreover, the spacy library is imported for advanced NLP tasks, including tokenization, part-of-speech tagging, and named entity recognition.\n","\n","Furthermore, the code imports the train_test_split function from the sklearn.model_selection module, which is useful for splitting datasets into training and validation sets during model development. Finally, the re module is imported for regular expression operations, offering powerful tools for pattern matching and text manipulation. Additionally, the gc module is imported for garbage collection, ensuring efficient memory management during code execution. Lastly, the warnings module is imported to handle and suppress any warnings that may arise during code execution, ensuring a clean and uninterrupted workflow. Overall, these import statements lay the groundwork for conducting various NLP tasks and fine-tuning transformer models for creative text generation."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T14:53:59.270575Z","iopub.status.busy":"2024-03-08T14:53:59.269875Z","iopub.status.idle":"2024-03-08T14:54:09.667341Z","shell.execute_reply":"2024-03-08T14:54:09.666370Z","shell.execute_reply.started":"2024-03-08T14:53:59.270541Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-03-08 14:54:04.371814: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-03-08 14:54:04.371896: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-03-08 14:54:04.373531: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["# Importing the Dataset class from the datasets library\n","from datasets import Dataset\n","\n","# Importing the DataCollatorWithPadding class from the transformers library\n","from transformers import DataCollatorWithPadding\n","\n","# Importing the Trainer and TrainingArguments classes, GPT2LMHeadModel and GPT2Tokenizer from the transformers library\n","from transformers import Trainer, TrainingArguments, GPT2LMHeadModel, GPT2Tokenizer\n","\n","# Importing the PorterStemmer class from the nltk.stem module\n","from nltk.stem import PorterStemmer\n","\n","# Importing the string module\n","import string\n","\n","# Importing the torch library for tensor computations\n","import torch\n","\n","# Importing the pandas library for data manipulation\n","import pandas as pd\n","\n","# Importing the SnowballStemmer class from the nltk.stem module\n","from nltk.stem import SnowballStemmer\n","\n","# Importing the spacy library for advanced NLP tasks\n","import spacy\n","\n","# Importing the train_test_split function from the sklearn.model_selection module\n","from sklearn.model_selection import train_test_split\n","\n","# Importing the re module for regular expression operations\n","import re\n","\n","\n","import spacy\n","\n","\n","import gc\n","\n","# Importing the warnings module to handle warnings\n","import warnings\n","\n","# Ignoring any warnings that might be generated when running the code\n","warnings.filterwarnings('ignore')\n"]},{"cell_type":"markdown","metadata":{},"source":["### Combining Data In one DF"]},{"cell_type":"markdown","metadata":{},"source":["The load_and_preprocess_data function serves as a critical component for preparing the dataset before it is used for training or further processing. The function takes two arguments, source_file and target_file, which represent the paths to the source and target files containing the data, respectively.\n","\n","Within the function, the data is loaded from the specified source and target files using the open function in read mode ('r'). The read method is then used to read the contents of each file line by line, and the splitlines method is applied to split the text into individual lines, effectively creating lists of strings for both the source and target data.\n","\n","Subsequently, the source and target data are combined into a single DataFrame named df using the pd.DataFrame constructor from the Pandas library. This DataFrame consists of three columns: 'source', 'target', and 'tag'. The 'source' column contains the text prompts, while the 'target' column contains the corresponding story continuations.\n","\n","To facilitate further analysis, the function extracts any tags present in the source data and creates a new column named 'tag' to store them. This is achieved by applying the re.findall function from the re module, which searches for patterns matching the specified regular expression (r'\\[(.*?)\\]') within each source text. These tags, if found, are then stored in the 'tag' column.\n","\n","After extracting the tags, the function proceeds to remove them from the source text using the re.sub function, which substitutes any occurrences of the tag pattern with an empty string (''). This effectively cleanses the source text of any tags, ensuring that only the raw text remains.\n","\n","Finally, any rows containing missing values (NaN) are removed from the DataFrame using the dropna method, ensuring data integrity and consistency.\n","\n","In summary, the load_and_preprocess_data function effectively loads, preprocesses, and structures the dataset, readying it for subsequent tasks such as model training or analysis. It encapsulates essential data preprocessing steps, including data loading, combining, tag extraction, tag removal, and handling missing values, ensuring that the dataset is well-prepared and suitable for further processing."]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T14:54:09.670087Z","iopub.status.busy":"2024-03-08T14:54:09.669340Z","iopub.status.idle":"2024-03-08T14:54:10.563955Z","shell.execute_reply":"2024-03-08T14:54:10.562744Z","shell.execute_reply.started":"2024-03-08T14:54:09.670054Z"},"trusted":true},"outputs":[],"source":["\n","def load_and_preprocess_data(source_file, target_file):\n","    # Load the data\n","    with open(source_file, 'r') as f:\n","        source = f.read().splitlines()\n","\n","    with open(target_file, 'r') as f:\n","        target = f.read().splitlines()\n","\n","    # Combine the data into one DataFrame\n","    df = pd.DataFrame({\n","        'source': source,\n","        'target': target\n","    })\n","\n","    # Extract tags and create a new column for them\n","    df['tag'] = df['source'].apply(lambda x: re.findall(r'\\[(.*?)\\]', x))\n","\n","    # Remove the tags from the 'source' column\n","    df['source'] = df['source'].apply(lambda x: re.sub(r'\\[(.*?)\\]', '', x))\n","\n","    # Remove any rows with missing values\n","    df = df.dropna()\n","    \n","    return df\n","\n","\n","# Load and preprocess the data\n","df=load_and_preprocess_data('/kaggle/input/writing-prompts/writingPrompts/valid.wp_source','/kaggle/input/writing-prompts/writingPrompts/valid.wp_target')\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T14:54:10.565993Z","iopub.status.busy":"2024-03-08T14:54:10.565528Z","iopub.status.idle":"2024-03-08T14:54:10.590288Z","shell.execute_reply":"2024-03-08T14:54:10.589148Z","shell.execute_reply.started":"2024-03-08T14:54:10.565950Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>source</th>\n","      <th>target</th>\n","      <th>tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Every person in the world undergoes a `` good...</td>\n","      <td>Clancy Marguerian , 154 , private first class ...</td>\n","      <td>[ WP ]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Space mining is on the rise . The Space tanke...</td>\n","      <td>„… and the little duckling will never be able ...</td>\n","      <td>[ WP ]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>`` I wo n't have time to explain all of this ...</td>\n","      <td>I wo n't have the time to explain all of this ...</td>\n","      <td>[ WP ]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Write about a song . Each sentence must start...</td>\n","      <td>* '' [ Sally ] ( https : //www.youtube.com/wat...</td>\n","      <td>[ CW ]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>You live in Skyrim . It is your job to keep l...</td>\n","      <td>Light is a marvelous thing . It alone can turn...</td>\n","      <td>[ EU ]</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>15615</th>\n","      <td>You are a teenager with the ability to measur...</td>\n","      <td>I decided to go with a 1-15 scale instead of 1...</td>\n","      <td>[ WP ]</td>\n","    </tr>\n","    <tr>\n","      <th>15616</th>\n","      <td>As your dying wish , you ask that your body i...</td>\n","      <td>The shock hit me hard as my lungs filled with ...</td>\n","      <td>[ WP ]</td>\n","    </tr>\n","    <tr>\n","      <th>15617</th>\n","      <td>A young child stumbles upon a serial killer d...</td>\n","      <td>`` Your mommy and daddy did n't raise you righ...</td>\n","      <td>[ WP ]</td>\n","    </tr>\n","    <tr>\n","      <th>15618</th>\n","      <td>Write from the perspective of a dog who think...</td>\n","      <td>She wants me to get into the car . It 's just ...</td>\n","      <td>[ WP ]</td>\n","    </tr>\n","    <tr>\n","      <th>15619</th>\n","      <td>When you die , you do n't go to the afterlife...</td>\n","      <td>Thomas loves science fiction , and is pleased ...</td>\n","      <td>[ WP ]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>15620 rows × 3 columns</p>\n","</div>"],"text/plain":["                                                  source  \\\n","0       Every person in the world undergoes a `` good...   \n","1       Space mining is on the rise . The Space tanke...   \n","2       `` I wo n't have time to explain all of this ...   \n","3       Write about a song . Each sentence must start...   \n","4       You live in Skyrim . It is your job to keep l...   \n","...                                                  ...   \n","15615   You are a teenager with the ability to measur...   \n","15616   As your dying wish , you ask that your body i...   \n","15617   A young child stumbles upon a serial killer d...   \n","15618   Write from the perspective of a dog who think...   \n","15619   When you die , you do n't go to the afterlife...   \n","\n","                                                  target     tag  \n","0      Clancy Marguerian , 154 , private first class ...  [ WP ]  \n","1      „… and the little duckling will never be able ...  [ WP ]  \n","2      I wo n't have the time to explain all of this ...  [ WP ]  \n","3      * '' [ Sally ] ( https : //www.youtube.com/wat...  [ CW ]  \n","4      Light is a marvelous thing . It alone can turn...  [ EU ]  \n","...                                                  ...     ...  \n","15615  I decided to go with a 1-15 scale instead of 1...  [ WP ]  \n","15616  The shock hit me hard as my lungs filled with ...  [ WP ]  \n","15617  `` Your mommy and daddy did n't raise you righ...  [ WP ]  \n","15618  She wants me to get into the car . It 's just ...  [ WP ]  \n","15619  Thomas loves science fiction , and is pleased ...  [ WP ]  \n","\n","[15620 rows x 3 columns]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"markdown","metadata":{},"source":["### Text Normalization: This includes converting all text to lower case, which can help ensure that your algorithm does not treat the same words in different cases as different words."]},{"cell_type":"markdown","metadata":{},"source":["In the following code snippet, several text normalization and cleansing operations are performed on the source and target columns of the DataFrame df. Let's break down each line and discuss its purpose:\n","\n","df['source'] = df['source'].str.lower(): This line converts all text in the 'source' column to lowercase using the str.lower() method. Converting text to lowercase helps standardize the text data and ensures consistency in subsequent processing steps, such as tokenization and modeling. It prevents the model from treating the same words with different cases as different entities.\n","\n","df['target'] = df['target'].str.lower(): Similar to the first line, this line converts all text in the 'target' column to lowercase, following the same rationale as above.\n","\n","df['target'] = df['target'].str.replace('„', ''): Here, the str.replace() method is used to remove any occurrences of the character '„' from the text in the 'target' column. This character may represent a specific type of quotation mark or symbol that is not relevant to the task at hand. Removing such characters helps clean the text data and ensures that the model focuses on meaningful information.\n","\n","df['target'] = df['target'].str.replace('”', ''): Similarly, this line removes any occurrences of the character '”' from the text in the 'target' column. This character might represent another type of quotation mark or special character that could interfere with subsequent processing or modeling tasks.\n","\n","df['target'] = df['target'].str.replace('< newlin >', ' '): In this line, the str.replace() method is used to replace the string '< newlin >' with a space (' ') in the text of the 'target' column. This operation likely aims to handle newline characters that were represented as '< newlin >' in the text data. Replacing them with spaces ensures that the text remains coherent and does not introduce unnecessary artifacts during further processing.\n","\n","Overall, these operations contribute to data cleaning and normalization, which are essential preprocessing steps in natural language processing tasks. By standardizing the text data and removing irrelevant characters or symbols, these operations help ensure that the dataset is well-prepared for subsequent analysis or modeling tasks, ultimately improving the quality and effectiveness of the downstream processes."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T14:54:10.593254Z","iopub.status.busy":"2024-03-08T14:54:10.592861Z","iopub.status.idle":"2024-03-08T14:54:10.821359Z","shell.execute_reply":"2024-03-08T14:54:10.820071Z","shell.execute_reply.started":"2024-03-08T14:54:10.593222Z"},"trusted":true},"outputs":[],"source":["df['source'] = df['source'].str.lower()\n","df['target'] = df['target'].str.lower()\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T14:54:10.823354Z","iopub.status.busy":"2024-03-08T14:54:10.823014Z","iopub.status.idle":"2024-03-08T14:54:10.944495Z","shell.execute_reply":"2024-03-08T14:54:10.943538Z","shell.execute_reply.started":"2024-03-08T14:54:10.823326Z"},"trusted":true},"outputs":[],"source":["df['target'] = df['target'].str.replace('„', '')\n","df['target'] = df['target'].str.replace('”', '')\n","df['target'] = df['target'].str.replace('< newlin >', ' ')\n"]},{"cell_type":"markdown","metadata":{},"source":["### Removing Punctuation: Punctuation can provide less value when training language models, and removing it can reduce the size of the vocabulary your model needs to learn."]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T14:54:10.946427Z","iopub.status.busy":"2024-03-08T14:54:10.946108Z","iopub.status.idle":"2024-03-08T14:54:12.322889Z","shell.execute_reply":"2024-03-08T14:54:12.322026Z","shell.execute_reply.started":"2024-03-08T14:54:10.946400Z"},"trusted":true},"outputs":[],"source":["df['source'] = df['source'].str.translate(str.maketrans('', '', string.punctuation))\n","df['target'] = df['target'].str.translate(str.maketrans('', '', string.punctuation))\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T14:54:12.325109Z","iopub.status.busy":"2024-03-08T14:54:12.324459Z","iopub.status.idle":"2024-03-08T14:54:12.342590Z","shell.execute_reply":"2024-03-08T14:54:12.341608Z","shell.execute_reply.started":"2024-03-08T14:54:12.325074Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>source</th>\n","      <th>target</th>\n","      <th>tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>every person in the world undergoes a  goodne...</td>\n","      <td>clancy marguerian  154  private first class of...</td>\n","      <td>[ WP ]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>space mining is on the rise  the space tanker...</td>\n","      <td>… and the little duckling will never be able t...</td>\n","      <td>[ WP ]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>i wo nt have time to explain all of this to ...</td>\n","      <td>i wo nt have the time to explain all of this t...</td>\n","      <td>[ WP ]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>write about a song  each sentence must start ...</td>\n","      <td>sally   https  wwwyoutubecomwatch  v6qyvil0...</td>\n","      <td>[ CW ]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>you live in skyrim  it is your job to keep li...</td>\n","      <td>light is a marvelous thing  it alone can turn ...</td>\n","      <td>[ EU ]</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>15615</th>\n","      <td>you are a teenager with the ability to measur...</td>\n","      <td>i decided to go with a 115 scale instead of 11...</td>\n","      <td>[ WP ]</td>\n","    </tr>\n","    <tr>\n","      <th>15616</th>\n","      <td>as your dying wish  you ask that your body is...</td>\n","      <td>the shock hit me hard as my lungs filled with ...</td>\n","      <td>[ WP ]</td>\n","    </tr>\n","    <tr>\n","      <th>15617</th>\n","      <td>a young child stumbles upon a serial killer d...</td>\n","      <td>your mommy and daddy did nt raise you right  ...</td>\n","      <td>[ WP ]</td>\n","    </tr>\n","    <tr>\n","      <th>15618</th>\n","      <td>write from the perspective of a dog who think...</td>\n","      <td>she wants me to get into the car  it s just so...</td>\n","      <td>[ WP ]</td>\n","    </tr>\n","    <tr>\n","      <th>15619</th>\n","      <td>when you die  you do nt go to the afterlife o...</td>\n","      <td>thomas loves science fiction  and is pleased t...</td>\n","      <td>[ WP ]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>15620 rows × 3 columns</p>\n","</div>"],"text/plain":["                                                  source  \\\n","0       every person in the world undergoes a  goodne...   \n","1       space mining is on the rise  the space tanker...   \n","2        i wo nt have time to explain all of this to ...   \n","3       write about a song  each sentence must start ...   \n","4       you live in skyrim  it is your job to keep li...   \n","...                                                  ...   \n","15615   you are a teenager with the ability to measur...   \n","15616   as your dying wish  you ask that your body is...   \n","15617   a young child stumbles upon a serial killer d...   \n","15618   write from the perspective of a dog who think...   \n","15619   when you die  you do nt go to the afterlife o...   \n","\n","                                                  target     tag  \n","0      clancy marguerian  154  private first class of...  [ WP ]  \n","1      … and the little duckling will never be able t...  [ WP ]  \n","2      i wo nt have the time to explain all of this t...  [ WP ]  \n","3         sally   https  wwwyoutubecomwatch  v6qyvil0...  [ CW ]  \n","4      light is a marvelous thing  it alone can turn ...  [ EU ]  \n","...                                                  ...     ...  \n","15615  i decided to go with a 115 scale instead of 11...  [ WP ]  \n","15616  the shock hit me hard as my lungs filled with ...  [ WP ]  \n","15617   your mommy and daddy did nt raise you right  ...  [ WP ]  \n","15618  she wants me to get into the car  it s just so...  [ WP ]  \n","15619  thomas loves science fiction  and is pleased t...  [ WP ]  \n","\n","[15620 rows x 3 columns]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T14:54:12.344206Z","iopub.status.busy":"2024-03-08T14:54:12.343812Z","iopub.status.idle":"2024-03-08T14:54:12.539555Z","shell.execute_reply":"2024-03-08T14:54:12.538691Z","shell.execute_reply.started":"2024-03-08T14:54:12.344177Z"},"trusted":true},"outputs":[],"source":["df['target'] = df['target'].str.replace('newline', '')\n","df['source'] = df['source'].str.replace('newline', '')"]},{"cell_type":"markdown","metadata":{},"source":["### Lemmatization: These techniques are used to reduce words to their root form. This can help your model generalize better to variations of words."]},{"cell_type":"markdown","metadata":{},"source":["In this section of the code, the Natural Language Processing (NLP) library spaCy is utilized for lemmatization, a process that involves reducing words to their base or root form. Here's a detailed explanation of each step:\n","\n","nlp = spacy.load('en_core_web_sm'): This line loads the English language model from spaCy. The model, 'en_core_web_sm', is a small English pipeline trained on web text data and includes components for tokenization, part-of-speech tagging, dependency parsing, and named entity recognition.\n","\n","df['source'] = df['source'].apply(lambda x: ' '.join([token.lemma_ for token in nlp(x)])): This line applies lemmatization to each word in the 'source' column of the DataFrame df. It uses a lambda function to iterate over each row (x) in the 'source' column. Within the lambda function, each sentence (x) is processed by spaCy's NLP pipeline (nlp(x)), which tokenizes the text into individual tokens. Then, for each token in the sentence, the lemma (base form) of the token is extracted (token.lemma_). Finally, the lemmatized tokens are joined back into a string using ' '.join().\n","\n","df['target'] = df['target'].apply(lambda x: ' '.join([token.lemma_ for token in nlp(x)])): Similar to the previous line, this line performs lemmatization on each word in the 'target' column of the DataFrame df. It follows the same approach of applying spaCy's NLP pipeline to each sentence (x), extracting lemmas for each token, and joining the lemmatized tokens back into a string."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nlp = spacy.load('en_core_web_sm')\n","\n","# Apply lemmatization to each word in the 'source' and 'target' columns of your DataFrame\n","df['source'] = df['source'].apply(lambda x: ' '.join([token.lemma_ for token in nlp(x)]))\n","df['target'] = df['target'].apply(lambda x: ' '.join([token.lemma_ for token in nlp(x)]))\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T14:54:12.542299Z","iopub.status.busy":"2024-03-08T14:54:12.541983Z","iopub.status.idle":"2024-03-08T14:54:12.552584Z","shell.execute_reply":"2024-03-08T14:54:12.551319Z","shell.execute_reply.started":"2024-03-08T14:54:12.542271Z"},"trusted":true},"outputs":[],"source":["train_df, valid_df = train_test_split(df, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T14:54:12.554491Z","iopub.status.busy":"2024-03-08T14:54:12.554122Z","iopub.status.idle":"2024-03-08T14:54:12.559652Z","shell.execute_reply":"2024-03-08T14:54:12.558661Z","shell.execute_reply.started":"2024-03-08T14:54:12.554463Z"},"trusted":true},"outputs":[],"source":["# Clear GPU memory\n","torch.cuda.empty_cache()\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T14:54:13.401356Z","iopub.status.busy":"2024-03-08T14:54:13.400246Z","iopub.status.idle":"2024-03-08T14:54:13.768159Z","shell.execute_reply":"2024-03-08T14:54:13.766906Z","shell.execute_reply.started":"2024-03-08T14:54:13.401310Z"},"trusted":true},"outputs":[{"data":{"text/plain":["287"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# Force garbage collection\n","gc.collect()\n"]},{"cell_type":"markdown","metadata":{},"source":["This section of the code involves preparing the dataset for training, tokenizing the input data, and initializing the model for fine-tuning. Here's a breakdown of each step:\n","\n","1. **Load Dataset Function**: The `load_dataset` function takes three arguments: `train_df`, `valid_df`, and `tokenizer`. It creates training and validation datasets from pandas DataFrames (`train_df` and `valid_df`) using the `Dataset.from_pandas` method provided by the Hugging Face Datasets library. This function also defines an inner function `tokenize_function` to tokenize the input examples. Within this function, the `tokenizer` is applied to the input sentences (`examples[\"source\"]`) with truncation and padding enabled to ensure uniform input lengths. Additionally, the function creates a new key-value pair in the tokenized inputs dictionary by copying the input IDs to the \"labels\" key. After defining the tokenization function, it maps this function to both the training and validation datasets using the `map` method with `batched=True`, which enables batch processing for efficiency.\n","\n","2. **Training Arguments**: The `TrainingArguments` object `training_args` is initialized to specify various training settings. These settings include the output directory for saving the trained model (`output_dir`), the number of training epochs (`num_train_epochs`), the batch size per GPU (`per_device_train_batch_size`), and the frequency of saving checkpoints (`save_steps`). Additionally, `save_total_limit` sets the maximum number of checkpoints to keep.\n","\n","3. **Tokenizer Initialization**: The GPT2 tokenizer (`tokenizer`) is initialized from the pre-trained \"distilgpt2\" model using `GPT2Tokenizer.from_pretrained`. This tokenizer is specifically designed for GPT-2 models and handles tokenization, special tokens, and padding.\n","\n","4. **Dataset Preparation**: The `load_dataset` function is called to prepare the training and validation datasets (`train_dataset` and `valid_dataset`) by tokenizing the input examples using the specified `tokenizer`. The tokenization function `tokenize_function` is applied to each dataset, ensuring that the input data is properly tokenized and formatted for training.\n","\n","5. **Data Collator Initialization**: The `DataCollatorWithPadding` object `data_collator` is initialized with the `tokenizer` to handle padding of input sequences during training. This collator ensures that sequences within each batch have the same length by padding shorter sequences with the appropriate padding token.\n","\n","6. **Model Initialization**: The GPT2 language model (`model`) is initialized from the pre-trained \"distilgpt2\" model using `GPT2LMHeadModel.from_pretrained`. This model is a variant of the GPT-2 architecture optimized for efficiency and reduced memory footprint while maintaining strong performance in language generation tasks.\n","\n","7. **Trainer Initialization**: Finally, the `Trainer` object `trainer` is initialized with the specified `model`, `training_args`, training dataset (`train_dataset`), evaluation dataset (`valid_dataset`), and data collator (`data_collator`). This trainer will be responsible for executing the fine-tuning process, utilizing the specified training arguments, and monitoring training progress."]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T14:54:14.646980Z","iopub.status.busy":"2024-03-08T14:54:14.645916Z","iopub.status.idle":"2024-03-08T16:34:26.639305Z","shell.execute_reply":"2024-03-08T16:34:26.637287Z","shell.execute_reply.started":"2024-03-08T14:54:14.646939Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2342aea9215e46bba3b6ac927a133db4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/13 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5316e69aca7c436e813988e95b62ffa6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/4 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["wandb version 0.16.4 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240308_145514-tducpgjl</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/one_man_army007/huggingface/runs/tducpgjl' target=\"_blank\">feasible-thunder-3</a></strong> to <a href='https://wandb.ai/one_man_army007/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/one_man_army007/huggingface' target=\"_blank\">https://wandb.ai/one_man_army007/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/one_man_army007/huggingface/runs/tducpgjl' target=\"_blank\">https://wandb.ai/one_man_army007/huggingface/runs/tducpgjl</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='18744' max='18744' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [18744/18744 1:38:36, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.166900</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.102000</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.096500</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.089600</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.085300</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.078800</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.075300</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.075600</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.072100</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.069900</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>0.069700</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>0.070000</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>0.061000</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>0.058200</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>0.055000</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>0.058300</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>0.055300</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>0.055600</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>0.052900</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>0.053700</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>0.051000</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>0.051000</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>0.050100</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>0.050600</td>\n","    </tr>\n","    <tr>\n","      <td>12500</td>\n","      <td>0.052400</td>\n","    </tr>\n","    <tr>\n","      <td>13000</td>\n","      <td>0.043600</td>\n","    </tr>\n","    <tr>\n","      <td>13500</td>\n","      <td>0.044400</td>\n","    </tr>\n","    <tr>\n","      <td>14000</td>\n","      <td>0.045000</td>\n","    </tr>\n","    <tr>\n","      <td>14500</td>\n","      <td>0.044400</td>\n","    </tr>\n","    <tr>\n","      <td>15000</td>\n","      <td>0.044600</td>\n","    </tr>\n","    <tr>\n","      <td>15500</td>\n","      <td>0.044400</td>\n","    </tr>\n","    <tr>\n","      <td>16000</td>\n","      <td>0.044400</td>\n","    </tr>\n","    <tr>\n","      <td>16500</td>\n","      <td>0.041700</td>\n","    </tr>\n","    <tr>\n","      <td>17000</td>\n","      <td>0.042800</td>\n","    </tr>\n","    <tr>\n","      <td>17500</td>\n","      <td>0.043400</td>\n","    </tr>\n","    <tr>\n","      <td>18000</td>\n","      <td>0.041600</td>\n","    </tr>\n","    <tr>\n","      <td>18500</td>\n","      <td>0.044800</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=18744, training_loss=0.06136047417823443, metrics={'train_runtime': 5995.7929, 'train_samples_per_second': 6.252, 'train_steps_per_second': 3.126, 'total_flos': 9795492586192896.0, 'train_loss': 0.06136047417823443, 'epoch': 3.0})"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["def load_dataset(train_df, valid_df, tokenizer):\n","    train_dataset = Dataset.from_pandas(train_df)\n","    valid_dataset = Dataset.from_pandas(valid_df)\n","\n","    def tokenize_function(examples):\n","        tokenized_inputs = tokenizer(examples[\"source\"], truncation=True, padding=\"max_length\")\n","        tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()\n","        return tokenized_inputs\n","\n","    train_dataset = train_dataset.map(tokenize_function, batched=True)\n","    valid_dataset = valid_dataset.map(tokenize_function, batched=True)\n","\n","    return train_dataset, valid_dataset\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./distilgpt2_story_gen\",\n","    overwrite_output_dir=True,\n","    num_train_epochs=3,\n","    per_device_train_batch_size=1,\n","    save_steps=10_000,\n","    save_total_limit=2,\n",")\n","\n","tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# Make sure to define train_df and valid_df before this line\n","train_dataset, valid_dataset = load_dataset(train_df, valid_df, tokenizer)\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=valid_dataset,\n","    data_collator=data_collator,\n",")\n","\n","trainer.train()\n"]},{"cell_type":"markdown","metadata":{},"source":["**Save Trained Model:**\n","The trained model is saved using the save_model method of the trainer object. The model is saved to the specified directory (./creative_writing_distilgpt2_story_gen) with the name pytorch_model.bin. This file contains the parameters and architecture of the trained model, allowing it to be loaded and utilized later without the need for retraining. Saving the model enables easy deployment in production environments or sharing with others for further experimentation.\n","\n","**Save Tokenizer:** \n","Similarly, the tokenizer used for tokenizing input sequences during training is saved using the save_pretrained method of the tokenizer object. The tokenizer is also saved to the same directory (./creative_writing_distilgpt2_story_gen) and stored in a separate file (tokenizer_config.json). This file contains the configuration settings of the tokenizer, including special tokens, vocabulary, and tokenization rules. Saving the tokenizer ensures consistency in tokenization when using the model for inference or further fine-tuning. Additionally, it allows for easy replication of the tokenization process across different environments or systems."]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T16:34:35.668590Z","iopub.status.busy":"2024-03-08T16:34:35.667611Z","iopub.status.idle":"2024-03-08T16:34:36.595876Z","shell.execute_reply":"2024-03-08T16:34:36.594800Z","shell.execute_reply.started":"2024-03-08T16:34:35.668536Z"},"trusted":true},"outputs":[{"data":{"text/plain":["('./creative_writing_distilgpt2_story_gen/tokenizer_config.json',\n"," './creative_writing_distilgpt2_story_gen/special_tokens_map.json',\n"," './creative_writing_distilgpt2_story_gen/vocab.json',\n"," './creative_writing_distilgpt2_story_gen/merges.txt',\n"," './creative_writing_distilgpt2_story_gen/added_tokens.json')"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# Save the trained model\n","trainer.save_model(\"./creative_writing_distilgpt2_story_gen\")\n","\n","# Save the tokenizer\n","tokenizer.save_pretrained(\"./creative_writing_distilgpt2_story_gen\")\n"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T16:40:02.103912Z","iopub.status.busy":"2024-03-08T16:40:02.103250Z","iopub.status.idle":"2024-03-08T16:40:05.051729Z","shell.execute_reply":"2024-03-08T16:40:05.050386Z","shell.execute_reply.started":"2024-03-08T16:40:02.103878Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Prompt: Once upon a time\n","Generated Text: Once upon a time  you are given a deal by a higher power that grants you eternal life  the catch  you have to kill one person every year  if you fail do so  even a minute too late  you will die <|endoftext|>\n","\n","Prompt: In a galaxy far far away\n","Generated Text: In a galaxy far far away  you ve been assigned to the first manned mission to the far future  but as you approach your destination  you notice that the mission has been abandoned <|endoftext|>\n","\n","Prompt: In the heart of the city\n","Generated Text: In the heart of the city  a man is banished to the wilderness for 20 years  write his diary entries for his first and last days of exile <|endoftext|>\n","\n"]}],"source":["from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","\n","# Load the trained model and tokenizer\n","model = GPT2LMHeadModel.from_pretrained(\"./creative_writing_distilgpt2_story_gen\")\n","tokenizer = GPT2Tokenizer.from_pretrained(\"./creative_writing_distilgpt2_story_gen\")\n","\n","# Define the prompts for text generation\n","prompts = [\"Once upon a time\", \"In a galaxy far far away\", \"In the heart of the city\"]\n","\n","# Generate text for each prompt\n","for prompt in prompts:\n","    # Encode the prompt to tokens\n","    encoded_prompt = tokenizer.encode(prompt, return_tensors=\"pt\")\n","    \n","    # Generate text\n","    output_sequences = model.generate(\n","        input_ids=encoded_prompt,\n","        attention_mask=encoded_prompt.ne(tokenizer.pad_token_id),  # Create attention mask\n","        pad_token_id=tokenizer.pad_token_id,  # Set pad_token_id\n","        max_length=200,\n","        temperature=0.9,\n","        top_k=1,\n","        top_p=0.9,\n","        repetition_penalty=1.0,\n","        do_sample=True,\n","        num_return_sequences=1,\n","    )\n","    \n","    # Decode the output sequences to text\n","    generated_text = tokenizer.decode(output_sequences[0], clean_up_tokenization_spaces=True)\n","    \n","    print(f\"Prompt: {prompt}\\nGenerated Text: {generated_text}\\n\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":611893,"sourceId":1094951,"sourceType":"datasetVersion"}],"dockerImageVersionId":30664,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
