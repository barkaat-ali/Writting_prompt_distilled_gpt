{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1094951,"sourceType":"datasetVersion","datasetId":611893},{"sourceId":7806509,"sourceType":"datasetVersion","datasetId":4571707}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Importing Libraies\n","metadata":{}},{"cell_type":"markdown","source":"The code snippet begins with a series of import statements, each serving a specific purpose in facilitating the subsequent tasks related to natural language processing (NLP) and model training. Firstly, the Dataset class is imported from the datasets library, which is part of the Hugging Face ecosystem. This class provides functionalities for handling datasets, including loading, processing, and manipulating data. Following this, the DataCollatorWithPadding class is imported from the transformers library. This class is instrumental in preparing input data for model training by padding sequences to ensure uniform length, a crucial step for batch processing.\n\nNext, several essential components for fine-tuning a pre-trained GPT-2 model are imported from the transformers library. These include the Trainer and TrainingArguments classes, which facilitate the training process by defining training parameters and orchestrating the training loop, respectively. Additionally, the GPT2LMHeadModel and GPT2Tokenizer classes are imported, representing the pre-trained GPT-2 model architecture and tokenizer, respectively. These components are pivotal for fine-tuning the GPT-2 model on custom data and generating text.\n\nThe code also imports essential libraries for data manipulation and analysis. Specifically, the torch library is imported for tensor computations, which are fundamental for neural network operations. Additionally, the pandas library is imported for efficient data manipulation, particularly for working with tabular data structures such as DataFrames. Moreover, the spacy library is imported for advanced NLP tasks, including tokenization, part-of-speech tagging, and named entity recognition.\n\nFurthermore, the code imports the train_test_split function from the sklearn.model_selection module, which is useful for splitting datasets into training and validation sets during model development. Finally, the re module is imported for regular expression operations, offering powerful tools for pattern matching and text manipulation. Additionally, the gc module is imported for garbage collection, ensuring efficient memory management during code execution. Lastly, the warnings module is imported to handle and suppress any warnings that may arise during code execution, ensuring a clean and uninterrupted workflow. Overall, these import statements lay the groundwork for conducting various NLP tasks and fine-tuning transformer models for creative text generation.","metadata":{}},{"cell_type":"code","source":"# Importing the Dataset class from the datasets library\nfrom datasets import Dataset\n\n# Importing the DataCollatorWithPadding class from the transformers library\nfrom transformers import DataCollatorWithPadding\n\n# Importing the Trainer and TrainingArguments classes, GPT2LMHeadModel and GPT2Tokenizer from the transformers library\nfrom transformers import Trainer, TrainingArguments, GPT2LMHeadModel, GPT2Tokenizer\n\n\n# Importing the string module\nimport string\n\n# Importing the torch library for tensor computations\nimport torch\n\n# Importing the pandas library for data manipulation\nimport pandas as pd\n\n\n# Importing the train_test_split function from the sklearn.model_selection module\nfrom sklearn.model_selection import train_test_split\n\n# Importing the re module for regular expression operations\nimport re\n\n\nimport gc\n\n# Importing the warnings module to handle warnings\nimport warnings\n\n# Ignoring any warnings that might be generated when running the code\nwarnings.filterwarnings('ignore')\n","metadata":{"execution":{"iopub.status.busy":"2024-03-10T10:54:35.651358Z","iopub.execute_input":"2024-03-10T10:54:35.652059Z","iopub.status.idle":"2024-03-10T10:55:05.878716Z","shell.execute_reply.started":"2024-03-10T10:54:35.652022Z","shell.execute_reply":"2024-03-10T10:55:05.877987Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-03-10 10:54:52.533557: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-10 10:54:52.533659: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-10 10:54:52.798359: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Combining Data In one DF","metadata":{}},{"cell_type":"markdown","source":"The load_and_preprocess_data function serves as a critical component for preparing the dataset before it is used for training or further processing. The function takes two arguments, source_file and target_file, which represent the paths to the source and target files containing the data, respectively.\n\nWithin the function, the data is loaded from the specified source and target files using the open function in read mode ('r'). The read method is then used to read the contents of each file line by line, and the splitlines method is applied to split the text into individual lines, effectively creating lists of strings for both the source and target data.\n\nSubsequently, the source and target data are combined into a single DataFrame named df using the pd.DataFrame constructor from the Pandas library. This DataFrame consists of three columns: 'source', 'target', and 'tag'. The 'source' column contains the text prompts, while the 'target' column contains the corresponding story continuations.\n\nTo facilitate further analysis, the function extracts any tags present in the source data and creates a new column named 'tag' to store them. This is achieved by applying the re.findall function from the re module, which searches for patterns matching the specified regular expression (r'\\[(.*?)\\]') within each source text. These tags, if found, are then stored in the 'tag' column.\n\nAfter extracting the tags, the function proceeds to remove them from the source text using the re.sub function, which substitutes any occurrences of the tag pattern with an empty string (''). This effectively cleanses the source text of any tags, ensuring that only the raw text remains.\n\nFinally, any rows containing missing values (NaN) are removed from the DataFrame using the dropna method, ensuring data integrity and consistency.\n\nIn summary, the load_and_preprocess_data function effectively loads, preprocesses, and structures the dataset, readying it for subsequent tasks such as model training or analysis. It encapsulates essential data preprocessing steps, including data loading, combining, tag extraction, tag removal, and handling missing values, ensuring that the dataset is well-prepared and suitable for further processing.","metadata":{}},{"cell_type":"code","source":"\ndef load_and_preprocess_data(source_file, target_file):\n    # Load the data\n    with open(source_file, 'r') as f:\n        source = f.read().splitlines()\n\n    with open(target_file, 'r') as f:\n        target = f.read().splitlines()\n\n    # Combine the data into one DataFrame\n    df = pd.DataFrame({\n        'source': source,\n        'target': target\n    })\n\n    # Extract tags and create a new column for them\n    df['tag'] = df['source'].apply(lambda x: re.findall(r'\\[(.*?)\\]', x))\n\n    # Remove the tags from the 'source' column\n    df['source'] = df['source'].apply(lambda x: re.sub(r'\\[(.*?)\\]', '', x))\n\n    # Remove any rows with missing values\n    df = df.dropna()\n    \n    return df\n\n\n# Load and preprocess the data\ndf=load_and_preprocess_data('/kaggle/input/dataaa/valid.wp_source',\n                            '/kaggle/input/dataaa/valid.wp_target')\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-10T10:55:05.880293Z","iopub.execute_input":"2024-03-10T10:55:05.881009Z","iopub.status.idle":"2024-03-10T10:55:07.265215Z","shell.execute_reply.started":"2024-03-10T10:55:05.880983Z","shell.execute_reply":"2024-03-10T10:55:07.263802Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-03-10T10:55:07.268573Z","iopub.execute_input":"2024-03-10T10:55:07.269309Z","iopub.status.idle":"2024-03-10T10:55:07.291449Z","shell.execute_reply.started":"2024-03-10T10:55:07.269274Z","shell.execute_reply":"2024-03-10T10:55:07.290569Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                                  source  \\\n0       Every person in the world undergoes a `` good...   \n1       Space mining is on the rise . The Space tanke...   \n2       `` I wo n't have time to explain all of this ...   \n3       Write about a song . Each sentence must start...   \n4       You live in Skyrim . It is your job to keep l...   \n...                                                  ...   \n15615   You are a teenager with the ability to measur...   \n15616   As your dying wish , you ask that your body i...   \n15617   A young child stumbles upon a serial killer d...   \n15618   Write from the perspective of a dog who think...   \n15619   When you die , you do n't go to the afterlife...   \n\n                                                  target     tag  \n0      Clancy Marguerian , 154 , private first class ...  [ WP ]  \n1      „… and the little duckling will never be able ...  [ WP ]  \n2      I wo n't have the time to explain all of this ...  [ WP ]  \n3      * '' [ Sally ] ( https : //www.youtube.com/wat...  [ CW ]  \n4      Light is a marvelous thing . It alone can turn...  [ EU ]  \n...                                                  ...     ...  \n15615  I decided to go with a 1-15 scale instead of 1...  [ WP ]  \n15616  The shock hit me hard as my lungs filled with ...  [ WP ]  \n15617  `` Your mommy and daddy did n't raise you righ...  [ WP ]  \n15618  She wants me to get into the car . It 's just ...  [ WP ]  \n15619  Thomas loves science fiction , and is pleased ...  [ WP ]  \n\n[15620 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source</th>\n      <th>target</th>\n      <th>tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Every person in the world undergoes a `` good...</td>\n      <td>Clancy Marguerian , 154 , private first class ...</td>\n      <td>[ WP ]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Space mining is on the rise . The Space tanke...</td>\n      <td>„… and the little duckling will never be able ...</td>\n      <td>[ WP ]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>`` I wo n't have time to explain all of this ...</td>\n      <td>I wo n't have the time to explain all of this ...</td>\n      <td>[ WP ]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Write about a song . Each sentence must start...</td>\n      <td>* '' [ Sally ] ( https : //www.youtube.com/wat...</td>\n      <td>[ CW ]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>You live in Skyrim . It is your job to keep l...</td>\n      <td>Light is a marvelous thing . It alone can turn...</td>\n      <td>[ EU ]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>15615</th>\n      <td>You are a teenager with the ability to measur...</td>\n      <td>I decided to go with a 1-15 scale instead of 1...</td>\n      <td>[ WP ]</td>\n    </tr>\n    <tr>\n      <th>15616</th>\n      <td>As your dying wish , you ask that your body i...</td>\n      <td>The shock hit me hard as my lungs filled with ...</td>\n      <td>[ WP ]</td>\n    </tr>\n    <tr>\n      <th>15617</th>\n      <td>A young child stumbles upon a serial killer d...</td>\n      <td>`` Your mommy and daddy did n't raise you righ...</td>\n      <td>[ WP ]</td>\n    </tr>\n    <tr>\n      <th>15618</th>\n      <td>Write from the perspective of a dog who think...</td>\n      <td>She wants me to get into the car . It 's just ...</td>\n      <td>[ WP ]</td>\n    </tr>\n    <tr>\n      <th>15619</th>\n      <td>When you die , you do n't go to the afterlife...</td>\n      <td>Thomas loves science fiction , and is pleased ...</td>\n      <td>[ WP ]</td>\n    </tr>\n  </tbody>\n</table>\n<p>15620 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Text Cleansing","metadata":{}},{"cell_type":"markdown","source":"In the following code snippet, several text cleansing operations are performed on the source and target columns of the DataFrame df. Let's break down each line and discuss its purpose:\n\ndf['target'] = df['target'].str.replace('„', ''): Here, the str.replace() method is used to remove any occurrences of the character '„' from the text in the 'target' column. This character may represent a specific type of quotation mark or symbol that is not relevant to the task at hand. Removing such characters helps clean the text data and ensures that the model focuses on meaningful information.\n\ndf['target'] = df['target'].str.replace('”', ''): Similarly, this line removes any occurrences of the character '”' from the text in the 'target' column. This character might represent another type of quotation mark or special character that could interfere with subsequent processing or modeling tasks.\n\ndf['target'] = df['target'].str.replace('< newlin >', ' '): In this line, the str.replace() method is used to replace the string '< newlin >' with a space (' ') in the text of the 'target' column. This operation likely aims to handle newline characters that were represented as '< newlin >' in the text data. Replacing them with spaces ensures that the text remains coherent and does not introduce unnecessary artifacts during further processing.\n\nOverall, these operations contribute to data cleaning and normalization, which are essential preprocessing steps in natural language processing tasks. By standardizing the text data and removing irrelevant characters or symbols, these operations help ensure that the dataset is well-prepared for subsequent analysis or modeling tasks, ultimately improving the quality and effectiveness of the downstream processes.","metadata":{}},{"cell_type":"code","source":"df['target'] = df['target'].str.replace('„', '')\ndf['target'] = df['target'].str.replace('”', '')\ndf['target'] = df['target'].str.replace('< newlin >', ' ')\n","metadata":{"execution":{"iopub.status.busy":"2024-03-10T10:55:12.794920Z","iopub.execute_input":"2024-03-10T10:55:12.795277Z","iopub.status.idle":"2024-03-10T10:55:12.897641Z","shell.execute_reply.started":"2024-03-10T10:55:12.795248Z","shell.execute_reply":"2024-03-10T10:55:12.896931Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-03-10T10:55:13.330848Z","iopub.execute_input":"2024-03-10T10:55:13.331135Z","iopub.status.idle":"2024-03-10T10:55:13.349218Z","shell.execute_reply.started":"2024-03-10T10:55:13.331113Z","shell.execute_reply":"2024-03-10T10:55:13.348235Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                                  source  \\\n0       Every person in the world undergoes a `` good...   \n1       Space mining is on the rise . The Space tanke...   \n2       `` I wo n't have time to explain all of this ...   \n3       Write about a song . Each sentence must start...   \n4       You live in Skyrim . It is your job to keep l...   \n...                                                  ...   \n15615   You are a teenager with the ability to measur...   \n15616   As your dying wish , you ask that your body i...   \n15617   A young child stumbles upon a serial killer d...   \n15618   Write from the perspective of a dog who think...   \n15619   When you die , you do n't go to the afterlife...   \n\n                                                  target     tag  \n0      Clancy Marguerian , 154 , private first class ...  [ WP ]  \n1      … and the little duckling will never be able t...  [ WP ]  \n2      I wo n't have the time to explain all of this ...  [ WP ]  \n3      * '' [ Sally ] ( https : //www.youtube.com/wat...  [ CW ]  \n4      Light is a marvelous thing . It alone can turn...  [ EU ]  \n...                                                  ...     ...  \n15615  I decided to go with a 1-15 scale instead of 1...  [ WP ]  \n15616  The shock hit me hard as my lungs filled with ...  [ WP ]  \n15617  `` Your mommy and daddy did n't raise you righ...  [ WP ]  \n15618  She wants me to get into the car . It 's just ...  [ WP ]  \n15619  Thomas loves science fiction , and is pleased ...  [ WP ]  \n\n[15620 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source</th>\n      <th>target</th>\n      <th>tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Every person in the world undergoes a `` good...</td>\n      <td>Clancy Marguerian , 154 , private first class ...</td>\n      <td>[ WP ]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Space mining is on the rise . The Space tanke...</td>\n      <td>… and the little duckling will never be able t...</td>\n      <td>[ WP ]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>`` I wo n't have time to explain all of this ...</td>\n      <td>I wo n't have the time to explain all of this ...</td>\n      <td>[ WP ]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Write about a song . Each sentence must start...</td>\n      <td>* '' [ Sally ] ( https : //www.youtube.com/wat...</td>\n      <td>[ CW ]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>You live in Skyrim . It is your job to keep l...</td>\n      <td>Light is a marvelous thing . It alone can turn...</td>\n      <td>[ EU ]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>15615</th>\n      <td>You are a teenager with the ability to measur...</td>\n      <td>I decided to go with a 1-15 scale instead of 1...</td>\n      <td>[ WP ]</td>\n    </tr>\n    <tr>\n      <th>15616</th>\n      <td>As your dying wish , you ask that your body i...</td>\n      <td>The shock hit me hard as my lungs filled with ...</td>\n      <td>[ WP ]</td>\n    </tr>\n    <tr>\n      <th>15617</th>\n      <td>A young child stumbles upon a serial killer d...</td>\n      <td>`` Your mommy and daddy did n't raise you righ...</td>\n      <td>[ WP ]</td>\n    </tr>\n    <tr>\n      <th>15618</th>\n      <td>Write from the perspective of a dog who think...</td>\n      <td>She wants me to get into the car . It 's just ...</td>\n      <td>[ WP ]</td>\n    </tr>\n    <tr>\n      <th>15619</th>\n      <td>When you die , you do n't go to the afterlife...</td>\n      <td>Thomas loves science fiction , and is pleased ...</td>\n      <td>[ WP ]</td>\n    </tr>\n  </tbody>\n</table>\n<p>15620 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df['target'] = df['target'].str.replace('newline', '')\ndf['source'] = df['source'].str.replace('newline', '')","metadata":{"execution":{"iopub.status.busy":"2024-03-10T10:55:14.422531Z","iopub.execute_input":"2024-03-10T10:55:14.423251Z","iopub.status.idle":"2024-03-10T10:55:14.632502Z","shell.execute_reply.started":"2024-03-10T10:55:14.423217Z","shell.execute_reply":"2024-03-10T10:55:14.631582Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_df, valid_df = train_test_split(df, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T10:55:15.822093Z","iopub.execute_input":"2024-03-10T10:55:15.822459Z","iopub.status.idle":"2024-03-10T10:55:15.834941Z","shell.execute_reply.started":"2024-03-10T10:55:15.822430Z","shell.execute_reply":"2024-03-10T10:55:15.834124Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Clear GPU memory\ntorch.cuda.empty_cache()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-10T10:55:16.906948Z","iopub.execute_input":"2024-03-10T10:55:16.907482Z","iopub.status.idle":"2024-03-10T10:55:16.911522Z","shell.execute_reply.started":"2024-03-10T10:55:16.907453Z","shell.execute_reply":"2024-03-10T10:55:16.910635Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Force garbage collection\ngc.collect()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-10T10:55:17.852472Z","iopub.execute_input":"2024-03-10T10:55:17.853274Z","iopub.status.idle":"2024-03-10T10:55:18.114913Z","shell.execute_reply.started":"2024-03-10T10:55:17.853244Z","shell.execute_reply":"2024-03-10T10:55:18.113975Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"181"},"metadata":{}}]},{"cell_type":"markdown","source":"This section of the code involves preparing the dataset for training, tokenizing the input data, and initializing the model for fine-tuning. Here's a breakdown of each step:\n\n1. **Load Dataset Function**: The `load_dataset` function takes three arguments: `train_df`, `valid_df`, and `tokenizer`. It creates training and validation datasets from pandas DataFrames (`train_df` and `valid_df`) using the `Dataset.from_pandas` method provided by the Hugging Face Datasets library. This function also defines an inner function `tokenize_function` to tokenize the input examples. Within this function, the `tokenizer` is applied to the input sentences (`examples[\"source\"]`) with truncation and padding enabled to ensure uniform input lengths. Additionally, the function creates a new key-value pair in the tokenized inputs dictionary by copying the input IDs to the \"labels\" key. After defining the tokenization function, it maps this function to both the training and validation datasets using the `map` method with `batched=True`, which enables batch processing for efficiency.\n\n2. **Training Arguments**: The `TrainingArguments` object `training_args` is initialized to specify various training settings. These settings include the output directory for saving the trained model (`output_dir`), the number of training epochs (`num_train_epochs`), the batch size per GPU (`per_device_train_batch_size`), and the frequency of saving checkpoints (`save_steps`). Additionally, `save_total_limit` sets the maximum number of checkpoints to keep.\n\n3. **Tokenizer Initialization**: The GPT2 tokenizer (`tokenizer`) is initialized from the pre-trained \"distilgpt2\" model using `GPT2Tokenizer.from_pretrained`. This tokenizer is specifically designed for GPT-2 models and handles tokenization, special tokens, and padding.\n\n4. **Dataset Preparation**: The `load_dataset` function is called to prepare the training and validation datasets (`train_dataset` and `valid_dataset`) by tokenizing the input examples using the specified `tokenizer`. The tokenization function `tokenize_function` is applied to each dataset, ensuring that the input data is properly tokenized and formatted for training.\n\n5. **Data Collator Initialization**: The `DataCollatorWithPadding` object `data_collator` is initialized with the `tokenizer` to handle padding of input sequences during training. This collator ensures that sequences within each batch have the same length by padding shorter sequences with the appropriate padding token.\n\n6. **Model Initialization**: The GPT2 language model (`model`) is initialized from the pre-trained \"distilgpt2\" model using `GPT2LMHeadModel.from_pretrained`. This model is a variant of the GPT-2 architecture optimized for efficiency and reduced memory footprint while maintaining strong performance in language generation tasks.\n\n7. **Trainer Initialization**: Finally, the `Trainer` object `trainer` is initialized with the specified `model`, `training_args`, training dataset (`train_dataset`), evaluation dataset (`valid_dataset`), and data collator (`data_collator`). This trainer will be responsible for executing the fine-tuning process, utilizing the specified training arguments, and monitoring training progress.","metadata":{}},{"cell_type":"code","source":"def load_dataset(train_df, valid_df, tokenizer):\n    train_dataset = Dataset.from_pandas(train_df)\n    valid_dataset = Dataset.from_pandas(valid_df)\n\n    def tokenize_function(examples):\n        tokenized_inputs = tokenizer(examples[\"source\"], truncation=True, padding=\"max_length\")\n        tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()\n        return tokenized_inputs\n\n    train_dataset = train_dataset.map(tokenize_function, batched=True)\n    valid_dataset = valid_dataset.map(tokenize_function, batched=True)\n\n    return train_dataset, valid_dataset\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./distilgpt2_story_gen\",\n    overwrite_output_dir=True,\n    num_train_epochs=3,\n    per_device_train_batch_size=1,\n    save_steps=10_000,\n    save_total_limit=2,\n    evaluation_strategy=\"steps\",\n    logging_steps=2000,\n)\n\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\ntokenizer.pad_token = tokenizer.eos_token\n\n# Make sure to define train_df and valid_df before this line\ntrain_dataset, valid_dataset = load_dataset(train_df, valid_df, tokenizer)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\nmodel = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset,\n    data_collator=data_collator,\n)\n\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-10T11:05:18.334869Z","iopub.execute_input":"2024-03-10T11:05:18.335570Z","iopub.status.idle":"2024-03-10T13:04:53.425655Z","shell.execute_reply.started":"2024-03-10T11:05:18.335541Z","shell.execute_reply":"2024-03-10T13:04:53.424514Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/13 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65119cb46a3c494dafd5dac5222a7421"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"929bec50a58749318d4b98ffce684674"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='18744' max='18744' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [18744/18744 1:59:20, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>2000</td>\n      <td>0.113100</td>\n      <td>0.078508</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.077600</td>\n      <td>0.066821</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.069200</td>\n      <td>0.059295</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.056800</td>\n      <td>0.055079</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.052900</td>\n      <td>0.052484</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.049200</td>\n      <td>0.049194</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>0.044900</td>\n      <td>0.048309</td>\n    </tr>\n    <tr>\n      <td>16000</td>\n      <td>0.043000</td>\n      <td>0.046752</td>\n    </tr>\n    <tr>\n      <td>18000</td>\n      <td>0.041100</td>\n      <td>0.046420</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"text":"IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n","name":"stderr","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=18744, training_loss=0.060090443541542665, metrics={'train_runtime': 7160.5537, 'train_samples_per_second': 5.235, 'train_steps_per_second': 2.618, 'total_flos': 9795492586192896.0, 'train_loss': 0.060090443541542665, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"**Save Trained Model:**\nThe trained model is saved using the save_model method of the trainer object. The model is saved to the specified directory (./creative_writing_distilgpt2_story_gen) with the name pytorch_model.bin. This file contains the parameters and architecture of the trained model, allowing it to be loaded and utilized later without the need for retraining. Saving the model enables easy deployment in production environments or sharing with others for further experimentation.\n\n**Save Tokenizer:** \nSimilarly, the tokenizer used for tokenizing input sequences during training is saved using the save_pretrained method of the tokenizer object. The tokenizer is also saved to the same directory (./creative_writing_distilgpt2_story_gen) and stored in a separate file (tokenizer_config.json). This file contains the configuration settings of the tokenizer, including special tokens, vocabulary, and tokenization rules. Saving the tokenizer ensures consistency in tokenization when using the model for inference or further fine-tuning. Additionally, it allows for easy replication of the tokenization process across different environments or systems.","metadata":{}},{"cell_type":"code","source":"# Save the trained model\ntrainer.save_model(\"./creative_writing_distilgpt2_story_gen\")\n\n# Save the tokenizer\ntokenizer.save_pretrained(\"./creative_writing_distilgpt2_story_gen\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:06:51.894705Z","iopub.execute_input":"2024-03-10T13:06:51.895115Z","iopub.status.idle":"2024-03-10T13:06:52.851748Z","shell.execute_reply.started":"2024-03-10T13:06:51.895086Z","shell.execute_reply":"2024-03-10T13:06:52.850657Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"('./creative_writing_distilgpt2_story_gen/tokenizer_config.json',\n './creative_writing_distilgpt2_story_gen/special_tokens_map.json',\n './creative_writing_distilgpt2_story_gen/vocab.json',\n './creative_writing_distilgpt2_story_gen/merges.txt',\n './creative_writing_distilgpt2_story_gen/added_tokens.json')"},"metadata":{}}]},{"cell_type":"markdown","source":"This Python script demonstrates how to generate creative text using a pre-trained GPT language model fine-tuned on a dataset for creative writing tasks. Here's a breakdown of the script:\n\n1. **Importing Libraries**: The script imports necessary libraries from the `transformers` package, including `GPT2Tokenizer` and `GPT2LMHeadModel`, which are used for tokenization and language model loading, respectively.\n\n2. **Loading the Model and Tokenizer**: It loads the fine-tuned GPT-2 language model and tokenizer from the specified directory (`\"./creative_writing_distilgpt2_story_gen\"`). These pre-trained models have been fine-tuned on a dataset suitable for creative writing tasks.\n\n3. **Defining Prompts**: Three prompts are defined to initiate the text generation process. These prompts set the initial context or theme for generating creative text.\n\n4. **Generating Text for Each Prompt**: For each prompt, the script performs the following steps:\n   - **Encoding the Prompt**: The prompt text is encoded into tokens using the tokenizer. The encoded prompt is then prepared for input to the model.\n   - **Generating Text**: The model generates text based on the encoded prompt. Various parameters control the generation process, such as maximum length, temperature (controlling randomness), top-k and top-p sampling, repetition penalty, and the number of sequences to generate.\n   - **Decoding the Output Sequences**: The generated text sequences are decoded back into human-readable text using the tokenizer.\n   - **Printing Results**: The prompt and the corresponding generated text are printed to the console for each iteration.\n\n5. **Output**: The generated text for each prompt is displayed, providing creative story continuations based on the given prompts.","metadata":{}},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2LMHeadModel\n\n# Load the trained model and tokenizer\nmodel = GPT2LMHeadModel.from_pretrained(\"./creative_writing_distilgpt2_story_gen\")\ntokenizer = GPT2Tokenizer.from_pretrained(\"./creative_writing_distilgpt2_story_gen\")\n\n# Define the prompts for text generation\nprompts = [\"Once upon a time\", \"In a galaxy far far away\", \"In the heart of the city\"]\n\n# Generate text for each prompt\nfor prompt in prompts:\n    # Encode the prompt to tokens\n    encoded_prompt = tokenizer.encode(prompt, return_tensors=\"pt\")\n    \n    # Generate text\n    output_sequences = model.generate(\n        input_ids=encoded_prompt,\n        attention_mask=encoded_prompt.ne(tokenizer.pad_token_id),  # Create attention mask\n        pad_token_id=tokenizer.pad_token_id,  # Set pad_token_id\n        max_length=500,\n        temperature=0.9,\n        top_k=1,\n        top_p=0.9,\n        repetition_penalty=1.0,\n        do_sample=True,\n        num_return_sequences=1,\n    )\n    \n    # Decode the output sequences to text\n    generated_text = tokenizer.decode(output_sequences[0], clean_up_tokenization_spaces=True)\n    \n    print(f\"Prompt: {prompt}\\nGenerated Text: {generated_text}\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:06:59.563892Z","iopub.execute_input":"2024-03-10T13:06:59.564295Z","iopub.status.idle":"2024-03-10T13:07:02.521421Z","shell.execute_reply.started":"2024-03-10T13:06:59.564265Z","shell.execute_reply":"2024-03-10T13:07:02.520422Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Prompt: Once upon a time\nGenerated Text: Once upon a time, you're given a deal by a higher power that grants you eternal life. The catch? You have to kill one person every year. If you fail do do do so, even a minute too late, you will die.<|endoftext|>\n\nPrompt: In a galaxy far far away\nGenerated Text: In a galaxy far far away, a young alien race is discovered and it's about to take over the galaxy. The problem is they've got a very different agenda than they originally anticipated.<|endoftext|>\n\nPrompt: In the heart of the city\nGenerated Text: In the heart of the city, a man is banished to the wilderness for 20 years. Write his diary entries for his first and last days of exile.<|endoftext|>\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}